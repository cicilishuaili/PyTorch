{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Tutorial: Named Entity Recognition and the Bi-LSTM Conditional Random Field Algorithm\n",
    "\n",
    "Tutorial Link:  https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A bit more basic is the Bi-Directional Recurrent Neural Network (a more complicated diagram will be shown below to augment, this is the general idea):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/bilstm_flow.png\" width=\"50%\">\n",
    "\n",
    "It does however show sequences that are embedded used to make predictions.  They are trained on both past as well as future information from the given data.  This will make more sense shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/anaconda/envs/py35/bin/python'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "{sys.executable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /anaconda/envs/py35/lib/python3.5/site-packages (0.0.8)\n",
      "Requirement already satisfied: bs4 in /anaconda/envs/py35/lib/python3.5/site-packages (0.0.1)\n",
      "Requirement already satisfied: requests in /anaconda/envs/py35/lib/python3.5/site-packages (2.18.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /anaconda/envs/py35/lib/python3.5/site-packages (from bs4) (4.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda/envs/py35/lib/python3.5/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda/envs/py35/lib/python3.5/site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/py35/lib/python3.5/site-packages (from requests) (2018.4.16)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda/envs/py35/lib/python3.5/site-packages (from requests) (1.22)\n",
      "\u001b[31mmxnet-model-server 0.4 requires mxnet-mkl>=1.2, which is not installed.\u001b[0m\n",
      "\u001b[31mblobxfer 1.3.1 has requirement requests==2.19.1, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mblobxfer 1.3.1 has requirement ruamel.yaml==0.15.41, but you'll have ruamel-yaml 0.15.35 which is incompatible.\u001b[0m\n",
      "\u001b[31mmxnet-model-server 0.4 has requirement onnx==1.1.1, but you'll have onnx 1.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! {sys.executable} -m pip install jdc bs4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc7e00de0b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports for ths tutorial\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import jdc\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* Definitions\n",
    "\n",
    "    * BLSTM\n",
    "    * CRF and potentials\n",
    "    * Viterbi\n",
    "\n",
    "* Helper Functions\n",
    "\n",
    "* Create the Network\n",
    "\n",
    "* Train\n",
    "\n",
    "* Evaluate\n",
    "\n",
    "* Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BLSTM (Bidirectional-Long Short-Term Memory)\n",
    "\n",
    "As we saw, an LSTM addresses the vanishing gradient problem of the generic RNN by adding cell state and more non-linear activation function layers to pass on or attenuate signals to varying degrees.  However, the main limitation of an LSTM is that it can **only account for context from the past**, that is, the hidden state, h_t, takes only past information as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Named Entity Recognition Task\n",
    "\n",
    "For the task of **Named Entity Recognition (NER)** is helpful to have context from past as well as the future, or left and right contexts.  This can be addressed with a BLSTM which is two LSTMs, one processing information in a forward fashion as we have already seen and another LSTM that processes the sequences in a reverse fashion giving the future context.  _That second LSTM is just reading the sentence in reverse._  \n",
    "\n",
    "The hidden states from both LSTMs are then concatenated into a final output layer or vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Random Field\n",
    "\n",
    "We don't have to stop at the output vector from the BLSTM!  We're not to our tag for the entity, yet.  We need to understand costs of moving from one tag to the next (or staying put on a tag, even).\n",
    "\n",
    "In a CRF, the parameters are simply the costs associated with transitioning from one tag to another in a sequence.  They are stored as a _transition matrix_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Say **B** is the tag for the beginning of a multi-word entity, **I** signifies that we are inside an entity (which could be one word) and **O** means we are outside an entity. \n",
    "\n",
    "Next, is an example of B-I-O schema labeling (by the way, there are a myriad of other schemas out there - see [Referenes](#references) for some more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Word | Schema Tag |\n",
    "| --- | --- |\n",
    "| Micheleen | I |\n",
    "| was | O |\n",
    "| born | O |\n",
    "| in | O |\n",
    "| North | B |\n",
    "| Carolina | I |\n",
    "| but | O |\n",
    "| grew | O |\n",
    "| up | O |\n",
    "| in | O |\n",
    "| Texas | I |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at an example transition matrix for the costs of moving from one tag to the next (remember our BLSTM is understanding both the forward and reverse ordering to get more accurate boundaries for the named entities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/fbc7992e8c9399bb713bbf63664bf34b13f091c5/images/crf_transition_matrix.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "The Viterbi algorithm, by finding the optimal path, can figure out for us what the final tag should be for the input.  We decode the results from the CRF, moving backwards through the Viterbi graph to get the final score.  It is often seen as the final step of a CRF.\n",
    "\n",
    "Here, let's see a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/viterbi.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting it All Together\n",
    "\n",
    "\n",
    "Here we have word embeddings as the data for the forward and reverse LSTMs.  The resulting forward vector (V_f) and backwards vector (V_b) are concatenated into a final vector (V_o) that feeds into the CRF layer and is decoded via the Viterbi algorithm into the optimal tag for that input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/blstm_crf_details.png\" width=\"70%\">\n",
    "\n",
    "\n",
    "<div  align=\"right\"><a href=\"https://www.sciencedirect.com/science/article/pii/S1532046417300977\">Reference</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    \"\"\"Return the argmax as a python int\"\"\"\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        seq - the sequence (array)\n",
    "        to_ix - the indices to which seqence values are converted (dict)\n",
    "        \n",
    "    Output:\n",
    "        Numerical tensor\n",
    "        \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    \"\"\"Compute log sum exp in a numerically stable way for the forward algorithm.\n",
    "    \n",
    "    Note:  torch.expand creates a new dimension and broadcasts the values into it.\"\"\"\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        \"\"\"Initialize network.\"\"\"\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim # represents both directions\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.  This is how the CRF stores\n",
    "        # costs.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag in the\n",
    "        # transition matrix of the CRF\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "def init_hidden(self):\n",
    "    \"\"\"Two tensors to hold hidden states, one for each\n",
    "    LSTM direction with dimensions of (num_layers, \n",
    "    minibatch, hidden_dim)\"\"\"\n",
    "    # Minibatch small because small dataset below\n",
    "    return (torch.randn(2, MINIBATCH_SIZE, self.hidden_dim // 2),\n",
    "            torch.randn(2, MINIBATCH_SIZE, self.hidden_dim // 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "\n",
    "def _forward_backwards_trick(self, feats):\n",
    "    \"\"\"Core magic of the Conditional Random Field.  \n",
    "    \n",
    "    Input:\n",
    "        The word embeddeding vectors for a sentence\n",
    "    \n",
    "    Since we’re using PyTorch to compute gradients for us, \n",
    "    we technically only need the forward part of the forward-backward \n",
    "    algorithm \"\"\"\n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    # The \"full\" function fills in with a default value\n",
    "    # Here we create a 1x3 matrix\n",
    "    init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "    \n",
    "    # init_alphas holds costs of moving from the START_TAG\n",
    "    # to B, I or O\n",
    "    # Let's set the cost of staying at the start tag\n",
    "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "    forward_var = init_alphas\n",
    "\n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        alphas_t = []  # The forward tensors at this timestep\n",
    "        for next_tag in range(self.tagset_size):\n",
    "            # broadcast (copy into new dim) the emission score: \n",
    "            # it is the same regardless of\n",
    "            # the previous tag\n",
    "            emit_score = feat[next_tag].view(\n",
    "                1, -1).expand(1, self.tagset_size)\n",
    "            # the ith entry of trans_score is the score of transitioning to\n",
    "            # next_tag from i\n",
    "            trans_score = self.transitions[next_tag].view(1, -1)\n",
    "            # The ith entry of next_tag_var is the value for the\n",
    "            # edge (i -> next_tag) before we do log-sum-exp\n",
    "            next_tag_var = forward_var + trans_score + emit_score\n",
    "            # The forward variable for this tag is log-sum-exp of all the\n",
    "            # scores.\n",
    "            alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "        # Concatenate the alphas_t (cost at timesteps) for feature\n",
    "        forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "def _get_lstm_features(self, sentence):\n",
    "    \"\"\"Compute output vector of BLSTM - used in \n",
    "    the forward pass of network\"\"\"\n",
    "    self.hidden = self.init_hidden()\n",
    "    embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "    lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "    lstm_feats = self.hidden2tag(lstm_out)\n",
    "    return lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "def _score_sentence(self, feats, tags):\n",
    "    \"\"\"Gives the score of a provided tag sequence\"\"\"\n",
    "    score = torch.zeros(1)\n",
    "    tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "    for i, feat in enumerate(feats):\n",
    "        score = score + \\\n",
    "            self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "    score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "def _viterbi_decode(self, feats):\n",
    "    \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
    "    Used in the forward pass of the network.\n",
    "\n",
    "    We take the maximum over the previous states as opposed to the sum. \n",
    "    Input:\n",
    "        loglikelihoods: torch tensor.\n",
    "    Output:\n",
    "        tuple. The first entry is the loglikelihood of this sequence. The second is \n",
    "        the most likely sequence of labels. \n",
    "    \"\"\"\n",
    "    backpointers = []\n",
    "\n",
    "    # Initialize the viterbi variables in log space\n",
    "    init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "    # forward_var at step i holds the viterbi variables for step i-1\n",
    "    forward_var = init_vvars\n",
    "    for feat in feats:\n",
    "        bptrs_t = []  # holds the backpointers for this step\n",
    "        viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "        for next_tag in range(self.tagset_size):\n",
    "            # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "            # previous step, plus the score of transitioning\n",
    "            # from tag i to next_tag.\n",
    "            # We don't include the emission scores here because the max\n",
    "            # does not depend on them (we add them in below)\n",
    "            next_tag_var = forward_var + self.transitions[next_tag]\n",
    "            best_tag_id = argmax(next_tag_var)\n",
    "            bptrs_t.append(best_tag_id)\n",
    "            viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "        # Now add in the emission scores, and assign forward_var to the set\n",
    "        # of viterbi variables we just computed\n",
    "        forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    # Transition to STOP_TAG\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    best_tag_id = argmax(terminal_var)\n",
    "    path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "    # Follow the back pointers to decode the best path.\n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "    # Pop off the start tag (we don't want to return that to the caller)\n",
    "    start = best_path.pop()\n",
    "    assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "def neg_log_likelihood(self, sentence, tags):\n",
    "    \"\"\"Calculate th negative log likelihood given a sequence and labels\"\"\"\n",
    "    feats = self._get_lstm_features(sentence)\n",
    "    forward_score = self._forward_backwards_trick(feats)\n",
    "    gold_score = self._score_sentence(feats, tags)\n",
    "    return forward_score - gold_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to BiLSTM_CRF\n",
    "\n",
    "def forward(self, sentence):\n",
    "    \"\"\"The forward pass function for training the network\"\"\"\n",
    "    # Get the emission scores (output layer) from the \n",
    "    # BiLSTM \n",
    "    lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "    # Find the best path, given the emission scores from BiLSTM\n",
    "    score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "    return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "MINIBATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make up some training data - this is from a NYTimes recipe\n",
    "# The ingrediants and equipment are labeled as entities here.\n",
    "training_data = [(\n",
    "    \"strain broth through a fine sieve (or a colander lined \\\n",
    "         with cheesecloth) into a separate container.\".split(),\n",
    "    \"O I O O O I O O I O O I O O B I\".split()\n",
    "), (\n",
    "    \"pour broth into pot with vegetables and heat to a simmer.\".split(),\n",
    "    \"O I O I O I O O O O O\".split()\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a lookup dict for word to index\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "# again, normally you would do more than 300 epochs, but we have\n",
    "# toy data\n",
    "for epoch in range(300):  \n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. A lot happens.  Run our forward pass to get features from BLSTM,\n",
    "        # run the CRF and get the negative log likelihoods and find the best \n",
    "        # \"path\" through sentence with the tags using the viterbi algorithm \n",
    "        # (also part of forward pass).\n",
    "        # BTW our dynamic computational graph is created with the forward pass\n",
    "        # Returns the forward score - ground truth score (our loss measure)\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients (backprop), and update the \n",
    "        # parameters by calling optimizer.step() - optimizer here is \n",
    "        # SGD for our CRF\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check predictions after training\n",
    "# No need to accumulate gradients because this is a validation\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))\n",
    "    print(training_data[0][0])\n",
    "# We got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "Let's test our model on an unseen sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Give some test data\n",
    "test_data = [(\n",
    "    \"serve immediately, in a tureen or from the pot, sprinkling \\\n",
    "        each serving with herbs\".split(),\n",
    "    \"O O O O I O O O I O O I O I\".split()\n",
    ")]\n",
    "\n",
    "# Create a lookup dict for word to index\n",
    "word_to_ix = {}\n",
    "for sentence, tags in test_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(test_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))\n",
    "    print(test_data[0][1])\n",
    "    print(test_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Use BeautifulSoup Python library and the Requests library to download a recipe of your choosing from https://cooking.nytimes.com/ and find it's steps section.\n",
    "\n",
    "\n",
    "To get started, you can:\n",
    "\n",
    "```python\n",
    "page = requests.get('')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "steps = soup.findAll(\"ol\", {\"class\": \"recipe-steps\"})\n",
    "```\n",
    "\n",
    "Instructions:\n",
    "1. Clean up the html tags and any strange characters.\n",
    "2. Label entities with YEDDA:  https://github.com/michhar/YEDDA.\n",
    "3. Save the labels as text and write code to identify entities as labels for the BiLSTM above.\n",
    "4. Run a BiLSTM on this data and validate the predictions against the original labels.\n",
    "5. Run some test data through your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Understanding Bidirectional RNN in PyTorch](https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66)\n",
    "2. [Conditional Random Field Tutorial in PyTorch](https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463)\n",
    "3. [Character-level neural network for biomedical named entity recognition](https://www.sciencedirect.com/science/article/pii/S1532046417300977)\n",
    "4.  [Other named entity tag schemas](https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the hint.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "############### For toggle button ##################\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the hint.\"></form>''')\n",
    "display(tag)\n",
    "\n",
    "############### Write code below ##################\n",
    "\n",
    "# HINT!  (cleaning text data)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get('https://cooking.nytimes.com/recipes/1018442-chicken-soup-from-scratch')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "steps = soup.findAll(\"ol\", {\"class\": \"recipe-steps\"})\n",
    "\n",
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    \"\"\"Function to clean up the html tags in data.\"\"\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    # Remove html tags\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = cleantext.replace('\\n', ' ').rstrip().strip()\n",
    "    # Remove special quotes\n",
    "    cleantext = cleantext.replace('“', '').replace('”', '')\n",
    "    cleantext = cleantext.lower()\n",
    "    return cleantext\n",
    "\n",
    "cleansteps = cleanhtml(str(steps[0]))\n",
    "cleansteps\n",
    "\n",
    "with open('sample_data.txt', 'w') as f:\n",
    "    f.write(cleansteps)\n",
    "    \n",
    "# This data is for updating training data or to use in the exercise\n",
    "with open('sample_data.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
