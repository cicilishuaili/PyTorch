{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What in the world is a dynamic computational graph\n",
    "\n",
    "Caution:  this is not an easy topic so if it doesn't make sense now keep reading about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward/Backwards\n",
    "* In training a NN there are a couple of steps:  the forward pass and the backwards pass (back propagation of gradients).\n",
    "  * In PyTorch `forward` and `backward` are in the same class `torch.autograd.Function`\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's see an example network (and train it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Do some imports\n",
    "import torch\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = torch.tensor([4.])\n",
    "\n",
    "# This is just a vector of tensors\n",
    "weights = [torch.tensor([i], requires_grad=True) for i in (2., 5., 9., 7.)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exercise:  Print the type of a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create the network\n",
    "\n",
    "Here we'll see the graph created on-the-fly and the forward pass\n",
    "\n",
    "**Note:  static graph frameworks predefine the graph (that then can not change later) and then run inputs through it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTANT:  When we create b, the graph creation begins!!!\n",
    "\n",
    "# The next three lines of code (b, c, d creation) are our\n",
    "# forward pass - when the inputs are processed into output\n",
    "\n",
    "# BEGIN COMPUTATIONAL GRAPH CREATION (some operations)\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "# END GRAPH CREATION\n",
    "\n",
    "# This is the loss\n",
    "L = (10 - d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run backprop and check the gradient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w1 w.r.t to L: -36.0\n",
      "Gradient of w2 w.r.t to L: -28.0\n",
      "Gradient of w3 w.r.t to L: -8.0\n",
      "Gradient of w4 w.r.t to L: -20.0\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w{} w.r.t to L: {}\".format(index, gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exercise:  run the above cell one more time and see what happens\n",
    "\n",
    "**Remember the computational graph is constructed in PyTorch at the time it executes (`backward()` is called). Two things must be done to run over and over**\n",
    "  * Clear the gradients\n",
    "  * Build (and possibly redefine) the network again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exercise:  re-run the \"Create the network\" section and then \"Run backprop...\" section.  Why do the gradients change?  How do you reset the gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## As you'll see later...but to round this out\n",
    "\n",
    "Let's update the weights and zero them (we'd do this before running the network again as would happen in training)\n",
    "\n",
    "Your update and reset will look like:\n",
    "```python\n",
    "# For fun let's say we had a learning rate of 1e-4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's put it all together to create, run, backwards prop, update weights, clear gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w1 w.r.t to L: -36.0\n",
      "Gradient of w2 w.r.t to L: -28.0\n",
      "Gradient of w3 w.r.t to L: -8.0\n",
      "Gradient of w4 w.r.t to L: -20.0\n"
     ]
    }
   ],
   "source": [
    "# Define the leaf nodes\n",
    "a = torch.tensor([4.])\n",
    "\n",
    "# This is just a vector of tensors\n",
    "weights = [torch.tensor([i], requires_grad=True) for i in (2., 5., 9., 7.)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "\n",
    "# IMPORTANT:  When we create b, the graph creation begins!!!\n",
    "\n",
    "# The next three lines of code (b, c, d creation) are our\n",
    "# forward pass - when the inputs are processed into output\n",
    "\n",
    "# BEGIN COMPUTATIONAL GRAPH CREATION (some operations)\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "# END GRAPH CREATION\n",
    "\n",
    "# This is the loss\n",
    "L = (10 - d)\n",
    "\n",
    "# Run the backwards propagation of gradients \n",
    "# (remember your chain rule for differentiation? Well PyTorch\n",
    "# takes care of this for you!)\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w{} w.r.t to L: {}\".format(index, gradient))\n",
    "\n",
    "# For fun let's say we had a learning rate of 1e-4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we've done one epoch!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages\n",
    "\n",
    "* Easier to debug that a static graph (we can modify our graph and easily check variables and gradients)\n",
    "* Since the network is created when ran it can be modified **on-the-fly** (very good for NLP where input lengths and output lengths may differ like in machine translation)\n",
    "* Reminiscent (as you'll see more later) of regular Python and object oriented programming - closer to what devs know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "1.  [Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec) by Ayoosh Kathuria\n",
    "2.  [PyTorch: Autograd example](https://github.com/jcjohnson/pytorch-examples#pytorch-autograd) by Justin Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
