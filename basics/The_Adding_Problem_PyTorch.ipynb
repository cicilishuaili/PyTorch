{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etMAh1TAxxec"
   },
   "source": [
    "# The Adding Problem\n",
    "\n",
    "The \"adding problem\" was original proposed by Schmidhuber and colleagues as an example of a sequential task that LSTM's are particularly well suited for: http://people.idsia.ch/~juergen/nipslstm/node4.html\n",
    "\n",
    "Here we will consider a slightly modified version where the sequence is of fixed length T and the sequence has two dimensions. The first dimension is a random number uniformly from 0 to 1 and the second dimension is always 0 except for two random positions and acts as a mask to add the numbers in the first dimension. \n",
    "\n",
    ">![The Adding Problem](https://minpy.readthedocs.io/en/latest/_images/adding_problem.png)\n",
    "\n",
    "As another example, the following sequence of length 5\n",
    "```\n",
    "{(0.443, 0), \n",
    " (0.112, 1), \n",
    " (0.950, 0), \n",
    " (0.839, 1), \n",
    " (0.142, 0)} \n",
    " ```\n",
    "\n",
    "yields 0.112 + 0.839 = 0.951 as the answer since the 2nd and 4th elmemts are added. \n",
    "\n",
    "Here we will compare several different cells in PyTorch to see how well they solve the adding problem. The cells we consider are:\n",
    "\n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "\n",
    "We will also consider a convolutional layer. Conv1D is not a recurrent layer, but has been shown to me useful for some sequential tasks. \n",
    "\n",
    "All methods will be compared using MSE on a held out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "dqSmie_KEun4",
    "outputId": "0faf0fa7-69b7-49b5-a60c-77b97c0041c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 30kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x597e0000 @  0x7fada90b61c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "squ9-CcuEwHF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v49i2wJgJoZm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k4-F5eWl6zI1",
    "outputId": "194e83a0-6819-4453-87fa-c1e7cc691657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU!\n"
     ]
    }
   ],
   "source": [
    "# use CUDA or not\n",
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "  print(\"using cuda!\")\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  print(\"using CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEuvUaYKynm5"
   },
   "source": [
    "## Data loading functions\n",
    "\n",
    "We will define some helper functions to generate our datasets. `generate_sequence` will genrate a single sequence whereas `get_set` returns multiple sequences (so a *dataset* of sequences).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKmR1v-YGRHj"
   },
   "outputs": [],
   "source": [
    "def generate_sequence(seq_len = 10):\n",
    "  ''' generate sequences\n",
    "  \n",
    "  Args:\n",
    "  -----\n",
    "  seq_len : int (default 10)\n",
    "    The length of the sequence\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  tuple of 3 numpy arrays x, z, y. x and z are 1D arrays and have same length\n",
    "  y is a float that is the target we want to predict (addition of x masked by z)\n",
    "  \n",
    "  Example:\n",
    "  --------\n",
    "  \n",
    "  >>> x_seq, z_seq, y_target = generate_sequence(seq_len = 100)\n",
    "  \n",
    "  '''\n",
    "  x = np.random.rand(seq_len)\n",
    "  z_p = np.arange(seq_len)\n",
    "  np.random.shuffle(z_p)\n",
    "  z = np.zeros(seq_len)\n",
    "  z[z_p[0]] = 1\n",
    "  z[z_p[1]] = 1\n",
    "  y = x[z_p[0]] + x[z_p[1]]\n",
    "  return x, z, y\n",
    "\n",
    "def get_set(num_examples = 100, seq_len = 10):\n",
    "  '''\n",
    "  Get the data set used for training/testing networks.\n",
    "  \n",
    "  Args:\n",
    "  -----\n",
    "  num_examples : int (default 100)\n",
    "    Number of sequences to generate\n",
    "  \n",
    "  seq_len : int (default 10)\n",
    "    The length of the sequence\n",
    "    \n",
    "  Returns:\n",
    "  --------\n",
    "  typle of length 2 where the first tuple is a numpy array of shape \n",
    "  num_examples x seq_len x 2 and the second tuple is length num_examples\n",
    "  \n",
    "  Example:\n",
    "  --------\n",
    "  \n",
    "  >>> X, y = get_set(num_examples=1000, seq_len = 50)\n",
    "  \n",
    "  '''\n",
    "  X_set, Z_set, y_set = [], [], []\n",
    "\n",
    "  for _ in range(num_examples):\n",
    "    x_example, z_example, y_example = generate_sequence(seq_len)\n",
    "    X_set.append(x_example)\n",
    "    Z_set.append(z_example)\n",
    "    y_set.append(y_example)\n",
    "    \n",
    "  X = np.zeros((num_examples,seq_len,2))\n",
    "  X[:,:,0] = np.array(X_set)\n",
    "  X[:,:,1] = np.array(Z_set)\n",
    "  return X, np.array(y_set)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1x7uKBP0ZNk"
   },
   "source": [
    "Lets see `get_set` in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0IANSgzLGTec",
    "outputId": "3c347a24-945d-4703-d705-6ba108823ee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_set(num_examples=100, seq_len = 10)\n",
    "X_test, y_test = get_set(num_examples=100, seq_len = 10)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES3lsroR0jpu"
   },
   "source": [
    "So for the input we have a 3D array that has shape \"num examples\" x \"sequence length\" x \"num features.\"\n",
    "\n",
    "\n",
    "Note that the datasets that `get_set` returns are Numpy arrays, but PyTorch recquires PyTorch tensors. We could of course convert these Numpy arrays to PyTorch arrays, and then do some booking with indices to keep track of going through different batches when doing batch updates on the network.\n",
    "\n",
    "But that is tedious and PyTorch offers the Dataset class that we can inherit from to keep all this bookkeeping for us. Below we define the `SequenceDataset` generator class that will be used for all our data handilng for PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kl0sTbk6UhJ"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, num_examples, seq_len):\n",
    "    self.num_examples = num_examples\n",
    "    self.seq_len = seq_len\n",
    "    \n",
    "    X, y = get_set(num_examples=self.num_examples, seq_len = self.seq_len)\n",
    "    self.X = torch.from_numpy(X).float()\n",
    "    self.y = torch.from_numpy(y).float()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "      self.X = self.X.to(device)\n",
    "      self.y = self.y.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.num_examples\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZmrufss26CF"
   },
   "source": [
    "Lets create a training and test set with 100 examples for each and sequence lengths of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPsIkWsTJxro"
   },
   "outputs": [],
   "source": [
    "train_set = SequenceDataset(num_examples=100, seq_len = 10)\n",
    "test_set = SequenceDataset(num_examples=100, seq_len = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxjZMwEy3IaT"
   },
   "source": [
    "We can use PyTorch's `DataLoader` to specify the the batches of data to load for training. Note that each of the 100 example sequences are independent, so we also shuffle the order of the different sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-Rj0yQu3prw"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-khUeqXF2_g"
   },
   "source": [
    "## RNN\n",
    "\n",
    "We will start solving the Adding Problem with a simple RNN (the *Elman Network*). The network will update its internal hidden state for every element in the sequence until we reach the end. When we reach the end, we pass the final hidden state through a fully connected linear layer to predict the target. This type of architecture is sometimes called *many-to-one* since we are taking \"many\" elements (a sequence) to a single element (the target).\n",
    "\n",
    "<center>\n",
    "![Many to one](https://i.stack.imgur.com/QCnpU.jpg)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhkM836zGW82"
   },
   "outputs": [],
   "source": [
    "class RNNAdder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):    \n",
    "        super(RNNAdder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=self.input_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state. The shape of the tensor is\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        # since our input has the batch dimension in the first dim, \n",
    "        # we just use x.size(0)        \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        _, h_f = self.rnn(x, h_0)\n",
    "        # we only care about the final hidden state. The intermediate values \n",
    "        # of the hidden state are discarded. We pass the final hidden state\n",
    "        # through the fully connected linear layer\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2ukZFPkJkNz"
   },
   "outputs": [],
   "source": [
    "rnn_adder = RNNAdder(hidden_size = 12, input_size = 2)\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    rnn_adder = rnn_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73HZrMRDKPz2"
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_adder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "uONrEQwF93-r",
    "outputId": "8a96c460-fb96-4fb4-b27c-bf568d02a78c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.13951006531715393\n",
      "CPU times: user 13.5 s, sys: 142 ms, total: 13.6 s\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "#     if use_cuda and torch.cuda.is_available():\n",
    "#       sequences = sequences.to(device)\n",
    "#       targets = targets.to(device)\n",
    "\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = rnn_adder(sequences)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YOBl0zXCIUa0",
    "outputId": "22e5f618-369b-4718-f228-6ff90a08f127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16553369164466858\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = rnn_adder(test_set.X)\n",
    "  test_mse = torch.mean((outputs - test_set.y)**2)\n",
    "print(test_mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BJIjhpEHwSW"
   },
   "source": [
    "## LSTM\n",
    "\n",
    "RNN's suffer from the vanishing gradient problem since creating the final hidden state is a result of updating the state through multiplications everytime a new element arrives in the sequence. LSTM's bypass this challenge by updating state additively. As a result, updaing gradients is much easier and longer memories can persist. Below is an `LSTMAdder` that is nearly identical to the `RNNAdder.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhr2amTvEshB"
   },
   "outputs": [],
   "source": [
    "class LSTMAdder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):    \n",
    "        super(LSTMAdder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "          c_0 = c_0.to(device)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        _, (h_f, c_f) = self.lstm(x, (h_0, c_0))\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp2NuPyIKFBN"
   },
   "outputs": [],
   "source": [
    "lstm_adder = LSTMAdder(hidden_size = 12, input_size = 2)\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    lstm_adder = lstm_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ay85qAOeKJTT"
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_adder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sEr3G2M0KLXA",
    "outputId": "170a4a01-da3a-478b-e690-0f6bb6b1856d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.2949182987213135\n",
      "loss is 0.000536929932422936\n",
      "loss is 4.7254226956283674e-05\n",
      "loss is 0.00031535691232420504\n",
      "loss is 0.00027109982329420745\n",
      "loss is 0.0002998627896886319\n",
      "loss is 4.688739136327058e-05\n",
      "loss is 0.0002934955118689686\n",
      "loss is 0.00018490737420506775\n",
      "loss is 8.480088581563905e-06\n",
      "CPU times: user 9min 26s, sys: 4.72 s, total: 9min 30s\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    # forward pass\n",
    "    outputs = lstm_adder(sequences)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VqCzc4weKPRL",
    "outputId": "2ff4234b-00d9-438e-c90a-86fd914d1846"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = lstm_adder(test_set.X)\n",
    "  test_mse = torch.mean((outputs - test_set.y)**2)\n",
    "print(test_mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJMFpFxOiKja"
   },
   "source": [
    "## ReLU RNN\n",
    "\n",
    "The idea of the ReLU RNN is to initialize the hidden state of the RNN with the identity matrix and the bias with 0 and use the ReLU activation function. Below we demonstrate how such an RNN can be implemented. The results are not as good as the LSTM but certainly better than the traditional Elman Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQGhGZalh5y3"
   },
   "outputs": [],
   "source": [
    "class ReLURNNAdder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):    \n",
    "        super(ReLURNNAdder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=self.input_size,\n",
    "                          hidden_size=self.hidden_size, nonlinearity = \"relu\",\n",
    "                          batch_first=True)\n",
    "        \n",
    "        torch.nn.init.zeros_(self.rnn.weight_ih_l0)\n",
    "        torch.nn.init.eye_(self.rnn.weight_hh_l0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state. The shape of the tensor is\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        # since our input has the batch dimension in the first dim, \n",
    "        # we just use x.size(0)        \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        _, h_f = self.rnn(x, h_0)\n",
    "        # we only care about the final hidden state. The intermediate values \n",
    "        # of the hidden state are discarded. We pass the final hidden state\n",
    "        # through the fully connected linear layer\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEOdCuQEXn2K"
   },
   "source": [
    "We could train this model as before but if we want to be fair in our comparisons,  we should train each adder for each epoch for each batch. This can help us control the differences in training procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6qt1LU7jJZH"
   },
   "outputs": [],
   "source": [
    "relu_rnn_adder = ReLURNNAdder(hidden_size = 12, input_size = 2)\n",
    "rnn_adder = RNNAdder(hidden_size = 12, input_size = 2)\n",
    "lstm_adder = LSTMAdder(hidden_size = 12, input_size = 2)\n",
    "\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    relu_rnn_adder = relu_rnn_adder.cuda(device)\n",
    "    rnn_adder = rnn_adder.cuda(device)\n",
    "    lstm_adder = lstm_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xov4xqRbjPNF"
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "relu_rnn_opt = torch.optim.Adam(relu_rnn_adder.parameters(), lr=0.01)\n",
    "rnn_opt = torch.optim.Adam(rnn_adder.parameters(), lr=0.01)\n",
    "lstm_opt = torch.optim.Adam(lstm_adder.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nw7s_T_0lH2F"
   },
   "outputs": [],
   "source": [
    "def update_model(model, optimizer, input_sequences, output_targets):\n",
    "  preds = model(input_sequences)\n",
    "  loss = criterion(preds, output_targets)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "kmSGfpk6jSeq",
    "outputId": "c3e63ee4-2db7-4d35-f035-22739086434e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM loss:7.84e-03 RNN loss:1.03e-01 ReLURNN loss:3.34e-01\n",
      "LSTM loss:2.30e-04 RNN loss:5.75e-02 ReLURNN loss:4.10e-02\n",
      "LSTM loss:1.67e-04 RNN loss:4.40e-03 ReLURNN loss:2.56e-02\n",
      "LSTM loss:5.46e-04 RNN loss:3.14e-03 ReLURNN loss:1.14e-02\n",
      "LSTM loss:4.40e-04 RNN loss:4.15e-04 ReLURNN loss:7.64e-03\n",
      "LSTM loss:4.65e-04 RNN loss:2.17e-03 ReLURNN loss:1.93e-02\n",
      "LSTM loss:8.30e-05 RNN loss:5.40e-03 ReLURNN loss:1.21e-03\n",
      "LSTM loss:1.09e-04 RNN loss:5.69e-04 ReLURNN loss:5.51e-04\n",
      "LSTM loss:3.47e-04 RNN loss:1.71e-04 ReLURNN loss:2.05e-03\n",
      "LSTM loss:9.31e-05 RNN loss:1.15e-03 ReLURNN loss:3.66e-03\n",
      "LSTM loss:1.52e-04 RNN loss:7.90e-04 ReLURNN loss:5.76e-03\n",
      "LSTM loss:4.05e-04 RNN loss:1.50e-02 ReLURNN loss:6.46e-03\n",
      "LSTM loss:3.09e-05 RNN loss:3.69e-04 ReLURNN loss:5.80e-04\n",
      "LSTM loss:7.03e-06 RNN loss:5.70e-04 ReLURNN loss:1.01e-03\n",
      "LSTM loss:6.47e-05 RNN loss:9.25e-04 ReLURNN loss:1.42e-03\n",
      "LSTM loss:1.15e-04 RNN loss:7.27e-04 ReLURNN loss:1.14e-04\n",
      "LSTM loss:2.26e-04 RNN loss:4.90e-04 ReLURNN loss:1.54e-03\n",
      "LSTM loss:1.80e-04 RNN loss:3.39e-04 ReLURNN loss:1.08e-03\n",
      "LSTM loss:1.08e-04 RNN loss:1.53e-02 ReLURNN loss:5.13e-05\n",
      "LSTM loss:2.66e-05 RNN loss:5.79e-04 ReLURNN loss:1.93e-03\n",
      "LSTM loss:3.04e-05 RNN loss:4.25e-04 ReLURNN loss:4.29e-04\n",
      "LSTM loss:5.21e-05 RNN loss:6.47e-03 ReLURNN loss:2.42e-04\n",
      "LSTM loss:1.23e-04 RNN loss:8.23e-05 ReLURNN loss:4.18e-04\n",
      "LSTM loss:7.68e-06 RNN loss:2.25e-02 ReLURNN loss:2.57e-04\n",
      "LSTM loss:1.02e-04 RNN loss:2.55e-04 ReLURNN loss:6.06e-04\n",
      "LSTM loss:3.14e-05 RNN loss:1.03e-03 ReLURNN loss:1.38e-03\n",
      "LSTM loss:1.33e-04 RNN loss:2.55e-02 ReLURNN loss:1.67e-02\n",
      "LSTM loss:6.53e-04 RNN loss:5.44e-04 ReLURNN loss:1.91e-03\n",
      "LSTM loss:6.17e-05 RNN loss:1.63e-03 ReLURNN loss:2.69e-04\n",
      "LSTM loss:1.43e-05 RNN loss:2.66e-03 ReLURNN loss:3.32e-04\n",
      "LSTM loss:7.88e-05 RNN loss:2.31e-03 ReLURNN loss:1.64e-03\n",
      "LSTM loss:5.18e-05 RNN loss:2.33e-04 ReLURNN loss:1.90e-04\n",
      "LSTM loss:4.93e-06 RNN loss:3.90e-04 ReLURNN loss:2.35e-03\n",
      "LSTM loss:1.19e-05 RNN loss:8.29e-04 ReLURNN loss:2.94e-04\n",
      "LSTM loss:1.00e-04 RNN loss:1.75e-05 ReLURNN loss:8.80e-04\n",
      "LSTM loss:3.81e-06 RNN loss:1.02e-02 ReLURNN loss:1.51e-03\n",
      "LSTM loss:6.50e-06 RNN loss:9.91e-03 ReLURNN loss:2.16e-03\n",
      "LSTM loss:1.62e-04 RNN loss:2.76e-03 ReLURNN loss:1.80e-04\n",
      "LSTM loss:1.31e-05 RNN loss:1.22e-02 ReLURNN loss:4.16e-04\n",
      "LSTM loss:1.18e-05 RNN loss:2.53e-04 ReLURNN loss:1.26e-04\n",
      "LSTM loss:7.48e-05 RNN loss:9.47e-04 ReLURNN loss:2.68e-03\n",
      "LSTM loss:6.57e-06 RNN loss:2.76e-04 ReLURNN loss:3.83e-04\n",
      "LSTM loss:5.21e-04 RNN loss:3.06e-03 ReLURNN loss:1.06e-03\n",
      "LSTM loss:8.31e-05 RNN loss:8.71e-04 ReLURNN loss:2.36e-04\n",
      "LSTM loss:2.78e-05 RNN loss:2.48e-03 ReLURNN loss:4.67e-05\n",
      "LSTM loss:8.69e-07 RNN loss:5.68e-03 ReLURNN loss:1.54e-04\n",
      "LSTM loss:6.81e-05 RNN loss:1.20e-02 ReLURNN loss:4.38e-04\n",
      "LSTM loss:7.17e-07 RNN loss:1.79e-03 ReLURNN loss:1.26e-04\n",
      "LSTM loss:3.93e-06 RNN loss:8.80e-03 ReLURNN loss:5.18e-05\n",
      "LSTM loss:6.73e-06 RNN loss:3.38e-03 ReLURNN loss:1.30e-03\n",
      "CPU times: user 5min 6s, sys: 1min 25s, total: 6min 32s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm_losses = []\n",
    "rnn_losses = []\n",
    "relu_rnn_losses = []\n",
    "for epoch in range(5000):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    \n",
    "    loss = update_model(relu_rnn_adder, relu_rnn_opt, sequences, targets)\n",
    "    relu_rnn_losses.append(loss)\n",
    "    \n",
    "    loss = update_model(rnn_adder, rnn_opt, sequences, targets)\n",
    "    rnn_losses.append(loss)\n",
    "\n",
    "    loss = update_model(lstm_adder, lstm_opt, sequences, targets)\n",
    "    lstm_losses.append(loss)\n",
    "\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"LSTM loss:{:.2e}\".format(lstm_losses[-1]) , \n",
    "          \"RNN loss:{:.2e}\".format(rnn_losses[-1]), \n",
    "          \"ReLURNN loss:{:.2e}\".format(relu_rnn_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4F_mZAZxjXT_"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = relu_rnn_adder(test_set.X)\n",
    "  relu_rnn_mse = torch.mean((outputs - test_set.y)**2)\n",
    "\n",
    "  outputs = rnn_adder(test_set.X)\n",
    "  rnn_mse = torch.mean((outputs - test_set.y)**2)\n",
    "\n",
    "  outputs = lstm_adder(test_set.X)\n",
    "  lstm_mse = torch.mean((outputs - test_set.y)**2)\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oVlRUMyJjvjA",
    "outputId": "77d046f8-9b6b-45ed-ef11-ccd83703d7eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006150761619210243"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hZ3IfA-aLQxy",
    "outputId": "56ac66b5-d986-4112-f6b5-ebf8798e1480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05440850928425789"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uXzjM82ALSLl",
    "outputId": "b80a3ef6-401c-4200-ef16-27364e3e6ca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014745582593604922"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_rnn_mse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvV1FYzGYr91"
   },
   "source": [
    "While the LSTM is still the superior adder, the RNN initialized with the identity matrix and using the ReLU function is definitely better than the traidtional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy2C1wItLTl3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "The-Adding-Problem-PyTorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
