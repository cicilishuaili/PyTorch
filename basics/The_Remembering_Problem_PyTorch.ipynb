{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etMAh1TAxxec"
   },
   "source": [
    "# The Remembering Problem\n",
    "\n",
    "The \"remembering probem\" is a variant of the \"adding problem\" proposed by Schmidhuber and colleagues as an example of a sequential task that LSTM's are particularly well suited for: http://people.idsia.ch/~juergen/nipslstm/node4.html\n",
    "\n",
    "\n",
    "![frequent words](https://blogs.sas.com/content/sastraining/files/2015/11/word_frequency.png)\n",
    "\n",
    "Data source: http://norvig.com/google-books-common-words.txt\n",
    "\n",
    "\n",
    "All methods will be compared using MSE on a held out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "dqSmie_KEun4",
    "outputId": "0149a656-8ce1-4c9f-ec1f-284f75dcd2ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 31kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x59e78000 @  0x7f4b0d3e81c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "squ9-CcuEwHF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v49i2wJgJoZm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k4-F5eWl6zI1",
    "outputId": "2dedfe02-daf9-4b5f-e99e-9de78d645ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU!\n"
     ]
    }
   ],
   "source": [
    "# use CUDA or not\n",
    "use_cuda = False\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "  print(\"using cuda!\")\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  print(\"using CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEuvUaYKynm5"
   },
   "source": [
    "## Data loading functions\n",
    "\n",
    "We will define some helper functions to generate our datasets. `generate_sequence` will genrate a single sequence whereas `get_set` returns multiple sequences (so a *dataset* of sequences).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVkyE3gT5wWI"
   },
   "outputs": [],
   "source": [
    "def generate_char_seq(seq_len = 10):\n",
    "  return ''.join(random.choice(string.ascii_lowercase) for _ \n",
    "                 in range(seq_len))\n",
    "\n",
    "def get_set(num_examples = 100, seq_len = 10):\n",
    "  one_hot_encoded = {}\n",
    "  for i, char in enumerate(list(string.ascii_lowercase)):\n",
    "    one_hot_encoded[char] = i\n",
    "\n",
    "  X_seqs = []\n",
    "  num_repeats = []\n",
    "\n",
    "  for _ in range(num_examples):\n",
    "    seq_example = generate_char_seq(seq_len)\n",
    "    X_seqs.append([one_hot_encoded[char] for char in list(seq_example)])\n",
    "    num_repeats.append(collections.Counter(seq_example).most_common(1)[0][1])\n",
    "    \n",
    "  return np.array(X_seqs), np.array(num_repeats)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1x7uKBP0ZNk"
   },
   "source": [
    "Lets see `get_set` in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gsJVFU_75wTp",
    "outputId": "e62ce919-4e75-421e-d4ef-04e785f7eb26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_set(num_examples=100, seq_len = 3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES3lsroR0jpu"
   },
   "source": [
    "So for the input we have a 2D array that has shape \"num examples\" x \"sequence length\" \n",
    "\n",
    "Note that the datasets that `get_set` returns are Numpy arrays, but PyTorch recquires PyTorch tensors. We could of course convert these Numpy arrays to PyTorch arrays, and then do some booking with indices to keep track of going through different batches when doing batch updates on the network.\n",
    "\n",
    "But that is tedious and PyTorch offers the Dataset class that we can inherit from to keep all this bookkeeping for us. Below we define the `SequenceDataset` generator class that will be used for all our data handilng for PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGH1kJTGGvFN"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, num_examples, seq_len):\n",
    "    self.num_examples = num_examples\n",
    "    self.seq_len = seq_len\n",
    "    \n",
    "    X, y = get_set(num_examples=self.num_examples, seq_len = self.seq_len)\n",
    "    self.X = torch.LongTensor(X)\n",
    "    self.y = torch.from_numpy(y).float()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "      self.X = self.X.to(device)\n",
    "      self.y = self.y.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.num_examples\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZmrufss26CF"
   },
   "source": [
    "Lets create a training and test set with 100 examples for each and sequence lengths of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPsIkWsTJxro"
   },
   "outputs": [],
   "source": [
    "train_set = SequenceDataset(num_examples=100, seq_len = 10)\n",
    "test_set = SequenceDataset(num_examples=100, seq_len = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxjZMwEy3IaT"
   },
   "source": [
    "We can use PyTorch's `DataLoader` to specify the the batches of data to load for training. Note that each of the 100 example sequences are independent, so we also shuffle the order of the different sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-Rj0yQu3prw"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-khUeqXF2_g"
   },
   "source": [
    "## RNN\n",
    "\n",
    "We will start solving the Remembering Problem with a simple RNN (the *Elman Network*). The network will update its internal hidden state for every element in the sequence until we reach the end. When we reach the end, we pass the final hidden state through a fully connected linear layer to predict the target. This type of architecture is sometimes called *many-to-one* since we are taking \"many\" elements (a sequence) to a single element (the target).\n",
    "\n",
    "<center>\n",
    "![Many to one](https://i.stack.imgur.com/QCnpU.jpg)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhkM836zGW82"
   },
   "outputs": [],
   "source": [
    "class RNNRemember(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embedding_size, input_size):    \n",
    "        super(RNNRemember, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, self.embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        #h_0 = Variable(torch.zeros(1, embedding_size, self.hidden_size))\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "\n",
    "        emb = self.embedding(x)\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        _, h_f = self.rnn(emb, h_0)\n",
    "        return self.fc(h_f).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2ukZFPkJkNz"
   },
   "outputs": [],
   "source": [
    "rnn_remember = RNNRemember(hidden_size = 50, embedding_size = 5, \n",
    "                           input_size = len(string.ascii_lowercase))\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    rnn_adder = rnn_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73HZrMRDKPz2"
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_remember.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "uONrEQwF93-r",
    "outputId": "2681ac89-685a-405f-cf38-8bf97040a6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 2.297874743817374e-05\n",
      "loss is 0.0032314001582562923\n",
      "loss is 0.0002122371079167351\n",
      "loss is 0.0012824522564187646\n",
      "loss is 0.006178750656545162\n",
      "loss is 3.2399691463069757e-06\n",
      "loss is 2.0392006263136864e-05\n",
      "loss is 0.00036697823088616133\n",
      "loss is 5.5320295359706506e-05\n",
      "loss is 3.730580647243187e-05\n",
      "CPU times: user 12.2 s, sys: 1.13 s, total: 13.3 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "      sequences = sequences.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = rnn_remember(sequences)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YOBl0zXCIUa0",
    "outputId": "02ddef5d-9eca-4798-a8ee-7cd7f24506de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724921464920044\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = rnn_remember(test_set.X)\n",
    "  test_mse = torch.mean((outputs - test_set.y)**2)\n",
    "print(test_mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BJIjhpEHwSW"
   },
   "source": [
    "## LSTM\n",
    "\n",
    "RNN's suffer from the vanishing gradient problem since creating the final hidden state is a result of updating the state through multiplications everytime a new element arrives in the sequence. LSTM's bypass this challenge by updating state additively. As a result, updaing gradients is much easier and longer memories can persist. Below is an `LSTMAdder` that is nearly identical to the `RNNAdder.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhr2amTvEshB"
   },
   "outputs": [],
   "source": [
    "class LSTMRemember(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size, embedding_size):    \n",
    "        super(LSTMRemember, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, self.embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "          c_0 = c_0.to(device)\n",
    "          \n",
    "                  \n",
    "        emb = self.embedding(x)\n",
    "        # Propagate input through LSTM\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        _, (h_f, c_f) = self.lstm(emb, (h_0, c_0))\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp2NuPyIKFBN"
   },
   "outputs": [],
   "source": [
    "lstm_remember = LSTMRemember(hidden_size = 50, embedding_size = 5, \n",
    "                           input_size = len(string.ascii_lowercase))\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    lstm_remember = lstm_remember.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ay85qAOeKJTT"
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_remember.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1751
    },
    "colab_type": "code",
    "id": "sEr3G2M0KLXA",
    "outputId": "54f44f60-9d4c-440a-b036-a47e80289b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.0024148006923496723\n",
      "loss is 5.1259557949379086e-05\n",
      "loss is 8.673283673488186e-07\n",
      "loss is 0.01033545471727848\n",
      "loss is 4.511162842391059e-06\n",
      "loss is 3.4083641367033124e-06\n",
      "loss is 1.2738404109313706e-07\n",
      "loss is 0.001695329905487597\n",
      "loss is 1.3969316370321394e-08\n",
      "loss is 3.6411762494026334e-11\n",
      "loss is 9.540596579427074e-08\n",
      "loss is 0.0002487917663529515\n",
      "loss is 8.247862126609107e-08\n",
      "loss is 0.0001593692577444017\n",
      "loss is 0.00023692134709563106\n",
      "loss is 9.572919952915981e-05\n",
      "loss is 0.001537151401862502\n",
      "loss is 1.7767004464985803e-05\n",
      "loss is 5.851800233358517e-05\n",
      "loss is 0.01244555227458477\n",
      "loss is 4.7574124550919805e-07\n",
      "loss is 1.535607623281976e-07\n",
      "loss is 5.798185043204285e-09\n",
      "loss is 1.6104791029647458e-06\n",
      "loss is 2.9206352337496355e-05\n",
      "loss is 0.00021576270228251815\n",
      "loss is 1.823918501031585e-05\n",
      "loss is 1.463759872422088e-06\n",
      "loss is 3.244131221435964e-07\n",
      "loss is 7.617662163283967e-07\n",
      "loss is 3.2981015465338714e-06\n",
      "loss is 0.00015684122627135366\n",
      "loss is 0.00039906747406348586\n",
      "loss is 4.359980721346801e-06\n",
      "loss is 1.5051261925691506e-06\n",
      "loss is 8.090628398349509e-07\n",
      "loss is 0.006153366062790155\n",
      "loss is 3.964851657656254e-06\n",
      "loss is 1.46279774071445e-06\n",
      "loss is 1.0325422863388667e-06\n",
      "loss is 2.6470418106327998e-06\n",
      "loss is 2.1209845613157086e-07\n",
      "loss is 0.00031983706867322326\n",
      "loss is 1.7023489817802329e-06\n",
      "loss is 0.0001979391381610185\n",
      "loss is 4.284917849872727e-06\n",
      "loss is 0.00013192601909395307\n",
      "loss is 7.105571057763882e-06\n",
      "loss is 9.80077384156175e-05\n",
      "loss is 9.861624903351185e-07\n",
      "loss is 7.644122206329484e-07\n",
      "loss is 5.124479503137991e-05\n",
      "loss is 6.34401567367604e-06\n",
      "loss is 5.4121846915222704e-06\n",
      "loss is 1.1079157502535963e-06\n",
      "loss is 3.845935680146795e-06\n",
      "loss is 7.871439265727531e-06\n",
      "loss is 3.1984203815227374e-05\n",
      "loss is 2.350204340473283e-05\n",
      "loss is 2.4890736312954687e-05\n",
      "loss is 4.598700797942001e-06\n",
      "loss is 8.386872423216118e-07\n",
      "loss is 5.951941420789808e-05\n",
      "loss is 0.0001547501451568678\n",
      "loss is 5.643801614496624e-06\n",
      "loss is 5.726407835027203e-05\n",
      "loss is 5.383664756664075e-05\n",
      "loss is 5.456994358610245e-07\n",
      "loss is 8.123821316985413e-06\n",
      "loss is 1.450675313208194e-06\n",
      "loss is 7.338223895203555e-06\n",
      "loss is 3.9508358895545825e-06\n",
      "loss is 9.414161468157545e-07\n",
      "loss is 1.0807769285747781e-05\n",
      "loss is 1.45134913509537e-06\n",
      "loss is 0.0001349289814243093\n",
      "loss is 1.5419965393448365e-06\n",
      "loss is 1.1195036222488852e-06\n",
      "loss is 0.000263199326582253\n",
      "loss is 8.936399353842717e-06\n",
      "loss is 7.776977145113051e-05\n",
      "loss is 4.487360001803609e-06\n",
      "loss is 3.698780233207799e-07\n",
      "loss is 1.9507995602907613e-05\n",
      "loss is 1.3757584156337543e-06\n",
      "loss is 3.975105209974572e-05\n",
      "loss is 5.574209467340552e-07\n",
      "loss is 2.5005886072904104e-06\n",
      "loss is 3.465488305209874e-07\n",
      "loss is 1.9109322693111608e-06\n",
      "loss is 9.262514595320681e-07\n",
      "loss is 1.3331397212823504e-06\n",
      "loss is 1.0502079021534882e-05\n",
      "loss is 1.6216225048992783e-05\n",
      "loss is 1.9019713590751053e-06\n",
      "loss is 3.553111014298338e-07\n",
      "loss is 1.258020574823604e-06\n",
      "loss is 1.4624414689023979e-05\n",
      "loss is 0.00015063528553582728\n",
      "loss is 1.5132764929148834e-05\n",
      "CPU times: user 3min 46s, sys: 20.6 s, total: 4min 7s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    # forward pass\n",
    "    outputs = lstm_remember(sequences)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VqCzc4weKPRL",
    "outputId": "5d425b04-4218-4132-fc35-d540102ed9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8073420524597168\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = lstm_remember(test_set.X)\n",
    "  test_mse = torch.mean((outputs - test_set.y)**2)\n",
    "print(test_mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJMFpFxOiKja"
   },
   "source": [
    "## ReLU RNN\n",
    "\n",
    "The idea of the ReLU RNN is to initialize the hidden state of the RNN with the identity matrix and the bias with 0 and use the ReLU activation function. Below we demonstrate how such an RNN can be implemented. The results are not as good as the LSTM but certainly better than the traditional Elman Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy2C1wItLTl3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "The-Remembering-Problem-PyTorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
